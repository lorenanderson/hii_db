{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b466e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import pickle\n",
    "import numpy as np\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)  # Show all columns\n",
    "pd.set_option('display.width', 1000)        # Avoid line wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38725705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_by_name(df1_0, df2_0, name1 = 'Name', name2 = 'Name', extension = '2', order_by = 'Year'):\n",
    "    \"\"\"\n",
    "    Matches rows from df1_1 to df2_0 based on name1 and name2, adding columns with an extension\n",
    "\n",
    "    Parameters:\n",
    "        df1 (pd.DataFrame): The main dataframe.\n",
    "        df2 (pd.DataFrame): The dataframe containing the rows to match.\n",
    "        name1 (str): The name of df1 on which to match\n",
    "        name2 (str): The name of df2 on which to match\n",
    "        extension (str): The string to append to new column names in matched_rows_df.\n",
    "        order_by (str): The column by which to order the matches (e.g., \"Year\"). If None, no ordering is done.\n",
    "\n",
    "    Returns:\n",
    "        matched_rows_df (pd.DataFrame): A dataframe with matched row data.\n",
    "        unmatched_rows_df (pd.DataFrame): A dataframe with rows from df2 that did not match.\n",
    "    \"\"\"\n",
    "    # Initialize lists for matched and unmatched rows\n",
    "    matched_rows = []\n",
    "    unmatched_rows = []\n",
    "\n",
    "    # copy dfs so the originals aren't modified\n",
    "    df1 = df1_0.copy()\n",
    "    df2 = df2_0.copy()\n",
    "\n",
    "    # Split 'Name' column by semicolons and replace NaN/None with empty list\n",
    "    df1['Name_Split'] = df1[name1].str.split(';').apply(lambda x: x if isinstance(x, list) else [])\n",
    "    df2['Name_Split'] = df2[name2].str.split(';').apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "    # rename\n",
    "    common_columns = df1.columns.intersection(df2.columns)\n",
    "    df2 = df2.rename(columns={col: f'{col}_{extension}' for col in common_columns})\n",
    "\n",
    "    # Iterate over each row in df1\n",
    "    for i, row in df1.iterrows():\n",
    "        df1_names_set = set(row['Name_Split'])\n",
    "\n",
    "        # Filter df2 rows where Name_Split matches df1 names\n",
    "        matching_rows = df2[df2['Name_Split'+'_' + extension].apply(lambda x: bool(df1_names_set & set(x)))]\n",
    "\n",
    "        # If there are matching rows, process the first match\n",
    "        if not matching_rows.empty:\n",
    "            if order_by and order_by in matching_rows.columns:\n",
    "                matching_rows = matching_rows.sort_values(by=order_by, ascending=False)\n",
    "            matched_row = matching_rows.iloc[0]\n",
    "            \n",
    "            # Use pd.concat() to merge row and matched_row (add suffix to columns of matched_row)\n",
    "            matched_row_info = pd.concat([row, matched_row])\n",
    "            matched_rows.append(matched_row_info)\n",
    "        else:\n",
    "            # If no match, keep the original row\n",
    "            matched_rows.append(row)\n",
    "            unmatched_rows.append(row)\n",
    "\n",
    "    # Return the matched and unmatched rows as a DataFrame\n",
    "    matched_rows_df = pd.DataFrame(matched_rows).reset_index(drop=True)\n",
    "    unmatched_rows_df = pd.DataFrame(unmatched_rows).reset_index(drop=True)\n",
    "    return(matched_rows_df, unmatched_rows_df)\n",
    "\n",
    "def match_by_distance(df1, df2, size = 0., extension = '2', order_by = 'Year'):\n",
    "    \"\"\"\n",
    "    Matches rows from df1 to df2 based on distance criteria and returns a list of matched rows.\n",
    "\n",
    "    Parameters:\n",
    "        df1 (pd.DataFrame): The main dataframe.  Assumes GLong and GLat columns.\n",
    "        df2 (pd.DataFrame): The dataframe containing the rows to match.  Assumes GLong and GLat columns.\n",
    "        size (float): The distance threshold in degrees to consider a match.\n",
    "        extension (str): The string to append to new column names in matched_rows_df.\n",
    "        order_by (str): The column by which to order the matches (e.g., \"Year\"). If None, no ordering is done.\n",
    "\n",
    "    Returns:\n",
    "        matched_rows_df (pd.DataFrame): A dataframe with matched row data.\n",
    "        unmatched_rows_df (pd.DataFrame): A dataframe with rows from df2 that did not match.\n",
    "    \"\"\"\n",
    "    # Initialize lists for matched and unmatched rows\n",
    "    matched_rows = []\n",
    "    unmatched_rows = []\n",
    "    \n",
    "    # Compute distances between df1 and df2\n",
    "    distances = np.sqrt((np.asarray(df1['GLong'].values)[:, np.newaxis] - \n",
    "                         np.asarray(df2['GLong'].values)[np.newaxis, :])**2 + \n",
    "                        (np.asarray(df1['GLat'].values)[:, np.newaxis] - \n",
    "                         np.asarray(df2['GLat'].values)[np.newaxis, :])**2)\n",
    "\n",
    "    # Determine where distances are within the input size\n",
    "    wh = distances < size\n",
    "    \n",
    "    # Iterate over each row in df1\n",
    "    for i, row in df1.iterrows():\n",
    "        # If there are matches within the size threshold\n",
    "        if np.sum(wh[i, :]):  \n",
    "            matched_indices = np.where(wh[i,:])[0]  # Get the indices of the matches\n",
    "            matching_rows = df2.iloc[matched_indices]\n",
    "            \n",
    "            # If order_by column exists, sort the matched rows by that column\n",
    "            if order_by and order_by in df2.columns:\n",
    "                matching_rows = matching_rows.sort_values(by=order_by, ascending=False)\n",
    "            matched_row = matching_rows.iloc[0]  # Pick the first match\n",
    "\n",
    "            # Calculate the distance to the first match\n",
    "            match_distance = distances[i, matched_indices[0]]\n",
    "            \n",
    "            # Use pd.concat() to merge row and matched_row (add suffix to columns of matched_row)\n",
    "            matched_row_info = pd.concat([row, matched_row.add_suffix(f'_{extension}')])\n",
    "            matched_rows.append(matched_row_info)\n",
    "\n",
    "            # Append the distance of the match to matched_row_info\n",
    "            matched_row_info['Match_Distance'] = match_distance\n",
    "        else:\n",
    "            # If no match, keep the original row\n",
    "            matched_rows.append(row)\n",
    "            unmatched_rows.append(row)\n",
    "\n",
    "    # Return the matched and unmatched rows as a DataFrame\n",
    "    matched_rows_df = pd.DataFrame(matched_rows).reset_index(drop=True)\n",
    "    unmatched_rows_df = pd.DataFrame(unmatched_rows).reset_index(drop=True)\n",
    "    return(matched_rows_df, unmatched_rows_df)\n",
    "\n",
    "def make_df(dir, columns):\n",
    "    \"\"\"\n",
    "    Makes large df from all .pkl files in \"dir\", with columns \"columns\"\n",
    "    \n",
    "    Parameters:\n",
    "        dir (str): The directory to pull from.\n",
    "        columns (list): The columns to include in the final datafram.\n",
    "        \n",
    "    Returns:\n",
    "        df (pd.DataFrame): A dataframe containing all the information from the directory.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Load each .pkl file into the dataframe\n",
    "    for filename in os.listdir(dir):\n",
    "        if filename.endswith('.pkl') and not filename.endswith('_2.pkl'):\n",
    "            print(filename)\n",
    "            file_path = os.path.join(dir, filename)\n",
    "        \n",
    "            # Load the .pkl file into a pandas DataFrame and include only specified columns\n",
    "            this_df = pd.read_pickle(file_path)[columns]\n",
    "\n",
    "            # Strip units by extracting the 'value' attribute from each column\n",
    "            this_df = this_df.map(lambda x: x.value if isinstance(x, u.Quantity) else x)\n",
    "            df = pd.concat([df, this_df], ignore_index=True)\n",
    "    return(df.reset_index(drop=True))\n",
    "\n",
    "def add_region(df, **kwargs):\n",
    "    \"\"\"\n",
    "    Adds a new region to the given df using provided information\n",
    "    in `kwargs`.  Computes RA, Dec, and GName.\n",
    "\n",
    "    Paramaters:\n",
    "    df (pd.DataFrame): The dataframe to which the new region data will be added.\n",
    "    kwargs (dict): A dictionary containing the necessary data for the new region.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add the new row to the DataFrame using the provided kwargs\n",
    "    df.loc[len(df)] = kwargs\n",
    "\n",
    "    # ICRS                                                                                                                                                                                                                                \n",
    "    galactic_coord = SkyCoord(l=kwargs['GLong'], b=kwargs['GLat'], frame='galactic', unit=(u.deg, u.deg))\n",
    "    icrs_coord = galactic_coord.transform_to('icrs')\n",
    "    df['RA (J2000)'] = icrs_coord.ra.value\n",
    "    df['Dec (J2000)'] = icrs_coord.dec.value\n",
    "    df['GName'] = generate_gname(df['GLong'], df['GLat'])\n",
    "                                                                                                                                                                                                                            \n",
    "def generate_gname(glong, glat):\n",
    "    \"\"\"\n",
    "    Creates a GName of the form Glll.lll+/-bb.bbb\n",
    "\n",
    "    Parameters:\n",
    "    glong (float): The Galactic longitude\n",
    "    glat (float): The Galactic latitude\n",
    "\n",
    "    Returns:\n",
    "    gname: GName of the form Glll.lll+/-bb.bbb\n",
    "    \"\"\"\n",
    "    glong_str = glong.apply(lambda x: f\"{np.floor(x * 1000) / 1000.:07.3f}\")\n",
    "    glat_str = glat.apply(lambda x: f\"{np.floor(x * 1000) / 1000.:+07.3f}\")\n",
    "\n",
    "    # Combine them into the desired format                                                                                                                                                                                            \n",
    "    return('G' + glong_str + glat_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba848c1",
   "metadata": {},
   "source": [
    "# Make df files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b88c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WISE\n",
    "version = 3.0\n",
    "str_version = f\"{version:.1f}\"\n",
    "dir_path = ''\n",
    "csv_file = f\"{dir_path}wise_hii_master_V{str_version}.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "WISE_df = pd.read_csv(csv_file)\n",
    "\n",
    "# Get rid of NaNs at the end of the file\n",
    "WISE_df = WISE_df.dropna(axis=1, how='all')\n",
    "\n",
    "# Fix groups\n",
    "WISE_df.loc[(WISE_df['Catalog'] == 'known') & (WISE_df['Group_Flag'] == 1), 'Catalog'] = 'group'\n",
    "\n",
    "# Make GName\n",
    "WISE_df['GName'] = generate_gname(WISE_df['GLong'], WISE_df['GLat'])\n",
    "\n",
    "# Make RA and Dec\n",
    "galactic_coord = SkyCoord(l=WISE_df['GLong'], b=WISE_df['GLat'], frame='galactic', unit=(u.deg, u.deg))\n",
    "icrs_coord = galactic_coord.transform_to('icrs')\n",
    "WISE_df['RA (J2000)'] = icrs_coord.ra.value\n",
    "WISE_df['Dec (J2000)'] = icrs_coord.dec.value\n",
    "\n",
    "# Split the 'Name' column in WISE_df by semicolons to handle multiple names\n",
    "WISE_df['Name_Split'] = WISE_df['Name'].apply(lambda x: x.split(';') if isinstance(x, str) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e71db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quireza.pkl\n",
      "hrds_diffuse.pkl\n",
      "balser.pkl\n",
      "shrds_pilot.pkl\n",
      "fich.pkl\n",
      "araya.pkl\n",
      "glostar_rrls.pkl\n",
      "gdigs.pkl\n",
      "balser15.pkl\n",
      "hrds_wise.pkl\n",
      "lockman89.pkl\n",
      "watson.pkl\n",
      "sewilo.pkl\n",
      "shrds.pkl\n",
      "lockman96.pkl\n",
      "arecibo_hrds.pkl\n",
      "fast_rrls.pkl\n",
      "hrds.pkl\n",
      "caswell87.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/49/w24p4_qx4gs31jfwhgysgjj00000gq/T/ipykernel_98859/3117866911.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n",
      "/var/folders/49/w24p4_qx4gs31jfwhgysgjj00000gq/T/ipykernel_98859/3117866911.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n",
      "/var/folders/49/w24p4_qx4gs31jfwhgysgjj00000gq/T/ipykernel_98859/3117866911.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n",
      "/var/folders/49/w24p4_qx4gs31jfwhgysgjj00000gq/T/ipykernel_98859/3117866911.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n",
      "/var/folders/49/w24p4_qx4gs31jfwhgysgjj00000gq/T/ipykernel_98859/3117866911.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n",
      "/var/folders/49/w24p4_qx4gs31jfwhgysgjj00000gq/T/ipykernel_98859/3117866911.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n",
      "/var/folders/49/w24p4_qx4gs31jfwhgysgjj00000gq/T/ipykernel_98859/3117866911.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n",
      "/var/folders/49/w24p4_qx4gs31jfwhgysgjj00000gq/T/ipykernel_98859/3117866911.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n",
      "/var/folders/49/w24p4_qx4gs31jfwhgysgjj00000gq/T/ipykernel_98859/3117866911.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n",
      "/var/folders/49/w24p4_qx4gs31jfwhgysgjj00000gq/T/ipykernel_98859/3117866911.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n",
      "/var/folders/49/w24p4_qx4gs31jfwhgysgjj00000gq/T/ipykernel_98859/3117866911.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n",
      "/var/folders/49/w24p4_qx4gs31jfwhgysgjj00000gq/T/ipykernel_98859/3117866911.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n",
      "/var/folders/49/w24p4_qx4gs31jfwhgysgjj00000gq/T/ipykernel_98859/3117866911.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n",
      "/var/folders/49/w24p4_qx4gs31jfwhgysgjj00000gq/T/ipykernel_98859/3117866911.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n",
      "/var/folders/49/w24p4_qx4gs31jfwhgysgjj00000gq/T/ipykernel_98859/3117866911.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that were dropped and their replacements:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m dropped_names:\n\u001b[1;32m     95\u001b[0m     dropped_row \u001b[38;5;241m=\u001b[39m dropped_rows[dropped_rows[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m name]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 96\u001b[0m     replacement_row \u001b[38;5;241m=\u001b[39m replacement_rows[replacement_rows[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m name]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Dropped - VLSR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdropped_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVLSR\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Author: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdropped_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthor\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3_2/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/anaconda3_2/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1714\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1714\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/anaconda3_2/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1647\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1645\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "# RRL_Surveys df\n",
    "pkl_directory = 'rrl_surveys/'\n",
    "\n",
    "# List of columns to keep\n",
    "rrl_columns = ['Name', 'GName', 'GLong', 'GLat', 'RA (J2000)', 'Dec (J2000)', 'TL', 'e_TL', 'FWHM', 'e_FWHM', \n",
    "               'VLSR', 'e_VLSR', 'VLSR_He', 'e_VLSR_He', 'FWHM_He', 'e_FWHM_He', \n",
    "               'TL_He', 'e_TL_He', 'VLSR_C', 'e_VLSR_C', 'FWHM_C', 'e_FWHM_C', 'TL_C', 'e_TL_C', \n",
    "               'Telescope', 'Resolution', 'Wavelength', 'Frequency', \n",
    "               'Author', 'Year'\n",
    "              ]\n",
    "\n",
    "# Load each .pkl file into the dataframe\n",
    "RRL_Surveys_df = pd.DataFrame()\n",
    "for filename in os.listdir(pkl_directory):\n",
    "    if filename.endswith('.pkl') and not filename.endswith('_2.pkl'):\n",
    "        file_path = os.path.join(pkl_directory, filename)\n",
    "        \n",
    "        # Load the .pkl file into a pandas DataFrame\n",
    "        df = pd.read_pickle(file_path)\n",
    "        df_filtered = df[rrl_columns]\n",
    "        print(filename)\n",
    "        # Remove rows where 'VLSR', 'FWHM', or 'TL' are NaN, None, or 0\n",
    "        #df_filtered = df_filtered[~df_filtered[['VLSR', 'FWHM', 'TL']].isin([None, 0]).any(axis=1) & \n",
    "        #                          df_filtered[['VLSR', 'FWHM', 'TL']].notna().all(axis=1)]\n",
    "\n",
    "\n",
    "        # Remove any columns with all NaN or empty entries before concatenation\n",
    "        # Removing e_Te column.  Causes error later\n",
    "        #df = df.dropna(axis=1, how='all')  # Drop columns where all values are NaN\n",
    "        RRL_Surveys_df = pd.concat([RRL_Surveys_df, df_filtered], ignore_index=True)\n",
    "\n",
    "\n",
    "######################## Deal with multiple RRLs from individual observations##########################################\n",
    "# Sort the DataFrame by 'GLong' and 'GLat' to ensure duplicates are adjacent\n",
    "RRL_Surveys_df = RRL_Surveys_df.sort_values(by=['GLong', 'GLat', 'TL'], ascending = [True, True, False])\n",
    "\n",
    "# For lines with multiple components, store additional line parameters in new variables\n",
    "RRL_Surveys_df['VLSR2'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['VLSR'].shift(-1)\n",
    "RRL_Surveys_df['e_VLSR2'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['e_VLSR'].shift(-1)\n",
    "RRL_Surveys_df['FWHM2'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['FWHM'].shift(-1)\n",
    "RRL_Surveys_df['e_FWHM2'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['e_FWHM'].shift(-1)\n",
    "RRL_Surveys_df['TL2'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['TL'].shift(-1)\n",
    "RRL_Surveys_df['e_TL2'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['e_TL'].shift(-1)\n",
    "RRL_Surveys_df['VLSR3'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['VLSR'].shift(-2)\n",
    "RRL_Surveys_df['e_VLSR3'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['e_VLSR'].shift(-2)\n",
    "RRL_Surveys_df['FWHM3'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['FWHM'].shift(-2)\n",
    "RRL_Surveys_df['e_FWHM3'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['e_FWHM'].shift(-2)\n",
    "RRL_Surveys_df['TL3'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['TL'].shift(-2)\n",
    "RRL_Surveys_df['e_TL3'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['e_TL'].shift(-2)\n",
    "RRL_Surveys_df['VLSR4'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['VLSR'].shift(-3)\n",
    "RRL_Surveys_df['e_VLSR4'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['e_VLSR'].shift(-3)\n",
    "RRL_Surveys_df['FWHM4'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['FWHM'].shift(-3)\n",
    "RRL_Surveys_df['e_FWHM4'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['e_FWHM'].shift(-3)\n",
    "RRL_Surveys_df['TL4'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['TL'].shift(-3)\n",
    "RRL_Surveys_df['e_TL4'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['e_TL'].shift(-3)\n",
    "\n",
    "\n",
    "####################### Remove duplicate entries that arise in the cases of multiple lines###############################\n",
    "# First, get the duplicate rows based on 'GLong' and 'GLat'\n",
    "duplicates = RRL_Surveys_df.duplicated(subset=['GLong', 'GLat'], keep='first')\n",
    "\n",
    "# For rows with matching 'GLong' and 'GLat', store the shortest name\n",
    "RRL_Surveys_df['Name'] = RRL_Surveys_df.groupby(['GLong', 'GLat'])['Name'].transform(lambda x: x.loc[x.str.len().idxmin()])\n",
    "\n",
    "# Remove the duplicate rows (keep the first occurrence)\n",
    "RRL_Surveys_df = RRL_Surveys_df[~duplicates]\n",
    "\n",
    "# Reset the index after dropping rows\n",
    "RRL_Surveys_df = RRL_Surveys_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "####### Now deal with the rare instances of when the same source was observed twice, with the same name, in different surveys#######\n",
    "# Sort the dataframe by 'Year' and 'GLong'\n",
    "RRL_Surveys_df_sorted = RRL_Surveys_df.sort_values(by=['Year', 'GName'], ascending=[False, True])\n",
    "\n",
    "# Drop duplicates, keeping the first (highest 'Year') for each 'Name'\n",
    "RRL_Surveys_df_unique = RRL_Surveys_df_sorted.drop_duplicates(subset='GName', keep='first')\n",
    "\n",
    "# Sort back into correct order\n",
    "RRL_Surveys_df = RRL_Surveys_df_unique.sort_values(by=['GLong', 'GLat'])\n",
    "\n",
    "# Find the rows that were dropped (those in sorted but not in unique)\n",
    "dropped_rows = RRL_Surveys_df_sorted.loc[~RRL_Surveys_df_sorted.index.isin(RRL_Surveys_df_unique.index)]\n",
    "\n",
    "# Find the rows that took their place\n",
    "dropped_names = dropped_rows['Name']\n",
    "\n",
    "# Get the rows that were kept and have the same names as the dropped ones\n",
    "replacement_rows = RRL_Surveys_df_unique[RRL_Surveys_df_unique['Name'].isin(dropped_names)]\n",
    "\n",
    "# Show the dropped rows\n",
    "print(\"Rows that were dropped and their replacements:\")\n",
    "for name in dropped_names:\n",
    "    dropped_row = dropped_rows[dropped_rows['Name'] == name].iloc[0]\n",
    "    replacement_row = replacement_rows[replacement_rows['Name'] == name].iloc[0]\n",
    "    \n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"  Dropped - VLSR: {dropped_row['VLSR']}, Author: {dropped_row['Author']}\")\n",
    "    print(f\"  Kept    - VLSR: {replacement_row['VLSR']}, Author: {replacement_row['Author']}\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "################# Other fixes######################################\n",
    "# Filter rows where the 'TL' column is NaN\n",
    "nan_TL_rows = RRL_Surveys_df[RRL_Surveys_df['TL'].isna()]\n",
    "\n",
    "# Print the 'Author' column for those rows\n",
    "print('These have missing line parameters')\n",
    "print(nan_TL_rows['Author'])\n",
    "\n",
    "# Remove blank spaces\n",
    "RRL_Surveys_df['Name'] = RRL_Surveys_df['Name'].str.replace(' ', '', regex=False)\n",
    "\n",
    "#################### Add other regions that aren't in main surveys################################\n",
    "# Update for 'G000.394-00.540'\n",
    "add_region(RRL_Surveys_df,  \n",
    "            VLSR=24.0, FWHM=39.0, Telescope='Effelsberg', Resolution=2.5, \n",
    "            Wavelength=6, Frequency=5, Lines='H110', \n",
    "            GLong=0.394, GLat=-0.540, \n",
    "            Name='G000.394-00.540', Author='Downes et al. (1980)')\n",
    "\n",
    "# Update for 'G000.489-00.668'\n",
    "add_region(RRL_Surveys_df, \n",
    "            VLSR=17.5, FWHM=24.0, Telescope='Effelsberg', Resolution=2.5, \n",
    "            Wavelength=6, Frequency=5, Lines='H110', \n",
    "            GLong=0.510, GLat=-0.051, Name='G000.510-00.051', Author='Downes et al. (1980)')\n",
    "\n",
    "# Update for 'G000.572-00.628'\n",
    "add_region(RRL_Surveys_df, \n",
    "            VLSR=20.0, FWHM=15.0, Telescope='Effelsberg', Resolution=2.5, \n",
    "            Wavelength=6, Frequency=5, Lines='H110', \n",
    "            GLong=0.572, GLat=-0.628, Name='G000.572-00.628', Author='Downes et al. (1980)')\n",
    "\n",
    "# Update for 'G001.149-00.062'\n",
    "add_region(RRL_Surveys_df, \n",
    "            VLSR=-17.0, FWHM=20.0, Telescope='Effelsberg', Resolution=2.5, \n",
    "            Wavelength=6, Frequency=5, Lines='H110', \n",
    "            GLong=1.149, GLat=-0.062, Name='G001.149-00.062', Author='Downes et al. (1980)')\n",
    "\n",
    "# Update for 'G000.361-00.780;S20'\n",
    "add_region(RRL_Surveys_df, \n",
    "            VLSR=20.0, FWHM=23.0, Telescope='Effelsberg', Resolution=2.5, \n",
    "            Wavelength=6, Frequency=5, Lines='H110', \n",
    "            GLong=0.361, GLat=-0.780, Name='G00.361-00.780;S20', Author='Downes et al. (1980)')\n",
    "\n",
    "# Update for 'G359.277-00.264'\n",
    "add_region(RRL_Surveys_df,\n",
    "            VLSR=-2.4, e_VLSR=0.6, FWHM=20.1, e_FWHM=1.5, Telescope='Effelsberg', \n",
    "            Resolution=0.8, Wavelength=2, Frequency=15, Lines='H76', \n",
    "            GLong=359.277, GLat=-0.264, Name='G359.277-00.264', Author='Wink, Altenhoff, & Metzger (1982)')\n",
    "\n",
    "# Update for 'G268.034-00.984'\n",
    "add_region(RRL_Surveys_df,\n",
    "            VLSR=1.8, e_VLSR=0.9, FWHM=36.5, e_FWHM=1.2, Telescope='Parkes', \n",
    "            Resolution=4.4, Wavelength=6, Frequency=5, Lines='H109', \n",
    "            GLong=268.034, GLat=-0.984, Name='G268.034-00.984', Author='Wilson et al. (1970)')\n",
    "\n",
    "# Update for 'G287.782-00.819'\n",
    "add_region(RRL_Surveys_df,\n",
    "            VLSR=-21.3, e_VLSR=0.8, FWHM=38.8, e_FWHM=6.0, Telescope='Parkes', \n",
    "            Resolution=4.4, Wavelength=6, Frequency=5, Lines='H109', \n",
    "            GLong=287.782, GLat=-0.819, Name='G287.782-00.819', Author='Wilson et al. (1970)')\n",
    "\n",
    "# Update for 'G333.605-00.095'\n",
    "add_region(RRL_Surveys_df,\n",
    "            VLSR=-53.7, e_VLSR=3.1, FWHM=44.7, e_FWHM=6.0, Telescope='Parkes', \n",
    "            Resolution=4.4, Wavelength=6, Frequency=5, Lines='H109', \n",
    "            GLong=333.605, GLat=-0.095, Name='G333.605-00.095', Author='Wilson et al. (1970)')\n",
    "\n",
    "# Update for 'WB43'\n",
    "add_region(RRL_Surveys_df,\n",
    "            VLSR=-0.8, e_VLSR=0.1, FWHM=15.3, e_FWHM=0.2, Telescope='GBT', \n",
    "            Resolution=2.5, Wavelength=6, Frequency=5, Lines='H103-H109', \n",
    "            GLong=92.668, GLat=3.069, Name='WB43', Author='Arvidsson, Kerton, & Foster (2009)')\n",
    "\n",
    "# Update for 'Galactic Center Lobe'\n",
    "# guessed on errors\n",
    "add_region(RRL_Surveys_df,\n",
    "            catalog='known', VLSR=2.4, e_VLSR=0.1, FWHM=20.5, e_FWHM=5, TL=33.2*0.001, e_TL = 5*0.001,\n",
    "            Telescope='GBT', Resolution=2.65, Wavelength=6, Frequency=5, Lines='H95-H117', \n",
    "            GLong=359.555, GLat=-0.040, Name='G359.555-00.040', Author='Anderson et al. (2024)')\n",
    "\n",
    "# Update for 'NR038;SHRDS078;G267.9-1.1'\n",
    "add_region(RRL_Surveys_df,\n",
    "            catalog='known', VLSR=3.0, e_VLSR=3.4, FWHM=35.1, e_FWHM=4.3, \n",
    "            Telescope='ATCA', Resolution=None, Wavelength=6, Frequency=5, Lines='H107', \n",
    "            GLong=267.942, GLat=-1.061, Name='G267.9-1.1', Author='Misanovic, Cram, & Green (2002)')\n",
    "\n",
    "# Update for 'ATCA352;SHRDS476;G313.8+0.7'\n",
    "add_region(RRL_Surveys_df,\n",
    "            catalog='known', VLSR=-53.4, e_VLSR=5.2, FWHM=26.5, e_FWHM=5.2, \n",
    "            Telescope='ATCA', Resolution=None, Wavelength=6, Frequency=5, Lines='H107', \n",
    "            GLong=313.788, GLat=0.712, Name='G313.8+0.7', Author='Misanovic, Cram, & Green (2002)')\n",
    "\n",
    "# Update for 'FA454'\n",
    "add_region(RRL_Surveys_df,\n",
    "            catalog='known', VLSR=-217.0, e_VLSR=0.09, FWHM=35.6, e_FWHM=0.22, \n",
    "            TL=17.1, e_TL=0.09, Telescope='GBT', Resolution=1.3, Wavelength=3, \n",
    "            Frequency=9, Lines='H87-H93', GLong=358.517, GLat=0.036, \n",
    "            Name='G358.517+0.036', Author='Anderson et al. (2020)')\n",
    "\n",
    "# Update for 'FA461'\n",
    "add_region(RRL_Surveys_df,\n",
    "            catalog='known', VLSR=-206.9, e_VLSR=0.10, FWHM=33.9, e_FWHM=0.23, \n",
    "            TL=20.8, e_TL=0.12, Telescope='GBT', Resolution=1.3, Wavelength=3, \n",
    "            Frequency=9, Lines='H87-H93', GLong=358.796, GLat=0.001, \n",
    "            Name='G358.796+0.001', Author='Anderson et al. (2020)')\n",
    "\n",
    "# Update for 'GS113'\n",
    "add_region(RRL_Surveys_df,\n",
    "            catalog='known', VLSR=-209.3, e_VLSR=0.22, FWHM=24.3, e_FWHM=0.28, \n",
    "            TL=5.8, e_TL=0.04, Telescope='GBT', Resolution=1.3, Wavelength=3, \n",
    "            Frequency=9, Lines='H87-H93', GLong=358.844, GLat=0.026, \n",
    "            Name='G358.844+0.026', Author='Anderson et al. (2020)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09aead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Te\n",
    "te_directory = 'te/'\n",
    "te_columns = [\"Name\", \"GName\", \"GLong\", \"GLat\", \"RA (J2000)\", \"Dec (J2000)\", \"Author\", \"Year\", \"Te\", \"e_Te\", \"y\", \"e_y\"]\n",
    "Te_df = make_df(te_directory, te_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a0fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Velocities\n",
    "multvel_directory = 'multiple_velocities/'\n",
    "multvel_columns = [\"Name\", \"GName\", \"GLong\", \"GLat\", \"RA (J2000)\", \"Dec (J2000)\", \"Author\", \"Year\", \"Real_VLSR\"]\n",
    "Multiple_Velocities_df = make_df(multvel_directory, multvel_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDAR\n",
    "kdar_directory = 'kdars/'\n",
    "kdar_columns = [\"Name\", \"GName\", \"GLong\", \"GLat\", \"RA (J2000)\", \"Dec (J2000)\", \"Author\", \"Year\", \"KDAR\", \"DMethod\"]\n",
    "KDARs_df = make_df(kdar_directory, kdar_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ccaa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fluxes\n",
    "fluxes_directory = 'fluxes/'\n",
    "IR_df = pd.read_csv(fluxes_directory + 'ir_fphot_NEW.csv')\n",
    "VGPS_df = pd.read_csv(fluxes_directory + 'vgps_fphot_NEW.csv')\n",
    "MAGPIS_df = pd.read_csv(fluxes_directory + 'magpis_fphot_NEW.csv')\n",
    "Fluxes_df = pd.merge(pd.merge(IR_df, VGPS_df, on='GName', how='left'), MAGPIS_df, on='GName', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radio continuum\n",
    "radio_continuum_directory = 'radio_continuum/'\n",
    "radio_continuum_columns = [\"GLong\", \"GLat\", \"RA (J2000)\", \"Dec (J2000)\", \"Author\", \"Year\"]\n",
    "Radio_Continuum_df = make_df(radio_continuum_directory, radio_continuum_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e9355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Molecules\n",
    "molecular_directory = 'molecular/'\n",
    "molecular_columns = [\"GLong\", \"GLat\", \"RA (J2000)\", \"Dec (J2000)\", \"VLSR\", \"Molecule\", \"Author\", \"Year\"]\n",
    "Molecular_df = make_df(molecular_directory, molecular_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1b1334-beb8-4a27-a2c7-24e3f939f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallax\n",
    "input_file = 'reid2019_merge.txt'\n",
    "columns = ['GName', 'Alias', 'RA (J2000)', 'Dec (J2000)', 'Parallax', 'e_Parallax', \n",
    "           'mux', 'e_mux', 'muy', 'e_muy', 'VLSR_Parallax', 'e_VLSR_Parallax', 'Author_Parallax']   \n",
    "Parallax_df = pd.read_csv(input_file, names=columns, skiprows=1, delimiter=r'\\s+')\n",
    "Parallax_df['GName'] = Parallax_df['GName'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de8b82",
   "metadata": {},
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match with RRL data\n",
    "WISE_Matched_df, _ = match_by_name(WISE_df, RRL_Surveys_df, extension = 'Observed', order_by = 'Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f691a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match with Te.  Since Wenger'19 uses the GName, have to finesse a little\n",
    "WISE_Matched_df['Name_hold'] = WISE_Matched_df['Name']\n",
    "WISE_Matched_df['Name'] = np.where(WISE_Matched_df['Name'].notnull(), WISE_Matched_df['Name'] + ';' + \n",
    "                                   WISE_Matched_df['GName'], WISE_Matched_df['GName'])\n",
    "WISE_Matched_df, _ = match_by_name(WISE_Matched_df, Te_df, extension = 'Te', order_by='Year')\n",
    "WISE_Matched_df['Name'] = WISE_Matched_df['Name_hold']\n",
    "del WISE_Matched_df['Name_hold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple velocities\n",
    "WISE_Matched_df, _ = match_by_name(WISE_Matched_df, Multiple_Velocities_df, extension = 'Real_VLSR', order_by = '_Multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDARs\n",
    "WISE_Matched_df, _ = match_by_name(WISE_Matched_df, KDARs_df, extension = 'KDAR', order_by = '_KDAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0106f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fluxes\n",
    "WISE_Matched_df = pd.merge(WISE_Matched_df, Fluxes_df, on='GName', how='left', suffixes=('', '_Flux'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07180503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radio continuum\n",
    "WISE_Matched_df, _ = match_by_distance(WISE_Matched_df, Radio_Continuum_df, size=30./3600, extension='Radio_Continuum', order_by='Year')\n",
    "for i, row in WISE_Matched_df.iterrows():\n",
    "    if (WISE_Matched_df.loc[i, 'Catalog']=='no_radio') and not (np.isnan(WISE_Matched_df['GLong_Radio_Continuum'][i])):\n",
    "        WISE_Matched_df.loc[i, 'Catalog']='observe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Molecules\n",
    "WISE_Matched_df, _ = match_by_distance(WISE_Matched_df, Molecular_df, size=30./3600, extension='Molecular', order_by='Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9cbe84-9fbd-443d-89ab-d0f78ad7071c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parallax\n",
    "WISE_Matched_df, unmatched = match_by_name(WISE_Matched_df, Parallax_df, name1 = 'Parallax_Name', name2 = 'GName', extension = 'Parallax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81bf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Spectra\n",
    "spectra_directory = 'spectra/'\n",
    "\n",
    "# Define a dictionary to map subdirectories to authors\n",
    "subdirectory_to_author = {\n",
    "    'hrds': 'Anderson et al. (2011)',\n",
    "    'hrds_diffuse': 'Anderson et al. (2017)',\n",
    "    'hrds_multvel': 'Anderson et al. (2015b)',\n",
    "    'hrds_wise': 'Anderson et al. (2015a)'\n",
    "}\n",
    "\n",
    "# initialize objects\n",
    "WISE_Matched_df['spectrum_x'] = None\n",
    "WISE_Matched_df['spectrum_y'] = None\n",
    "\n",
    "# Loop through all the subdirectories in the root directory\n",
    "for survey in os.listdir(spectra_directory):\n",
    "    survey_path = os.path.join(spectra_directory, survey)\n",
    "    \n",
    "    # Check if the current item is a subdirectory\n",
    "    if os.path.isdir(survey_path):\n",
    "        # Loop through all the text files in the subdirectory\n",
    "        for filename in os.listdir(survey_path):\n",
    "            if filename.endswith(\"_output.txt\"):  # Check for .txt files\n",
    "                name = filename.split('_')[0]\n",
    "                \n",
    "                # Check if the name matches any value in the 'Name_Split' column\n",
    "                for index, row in WISE_Matched_df.iterrows():\n",
    "                    # Check if 'name' is in the list of names in the 'Name_Split' column for the current row\n",
    "                    if isinstance(row['Name_Split'], list) and name in row['Name_Split']:\n",
    "                        # Read the x and y data from the text file\n",
    "                        file_path = os.path.join(survey_path, filename)\n",
    "                        data = pd.read_csv(file_path, delimiter=',', skiprows=1, names=['spectrum_x', 'spectrum_y'])\n",
    "                        \n",
    "                        # Add the x and y values to the matching row\n",
    "                        WISE_Matched_df.at[index, 'spectrum_x'] = data['spectrum_x'].values*u.km/u.s\n",
    "                        WISE_Matched_df.at[index, 'spectrum_y'] = data['spectrum_y'].values/1000 * u.K\n",
    "\n",
    "                        # Add the author column\n",
    "                        WISE_Matched_df.loc[index, 'Author_Spectrum'] = subdirectory_to_author.get(survey)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df07c73",
   "metadata": {},
   "source": [
    "# Various Fixes and Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175bdf4e-e131-4aef-9801-c984a32ea4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### group catalog################################################\n",
    "WISE_Matched_df.loc[(WISE_df['Catalog'] == 'observe') & (WISE_Matched_df['Group_Flag'] == 1), 'Catalog'] = 'group'\n",
    "WISE_Matched_df.loc[(WISE_df['Catalog'] == 'sharpless') & (WISE_Matched_df['Group_Flag'] == 1), 'Catalog'] = 'group'\n",
    "\n",
    "# Check that all associated sources have a group designation\n",
    "print(len(WISE_Matched_df[(WISE_Matched_df['Group_Flag'] == 1) & (WISE_Matched_df['Group'] == '')]), ' sources lacking a group name.')\n",
    "\n",
    "# Add group velocity and distance parameters\n",
    "WISE_Matched_df['Group_VLSR'] = None\n",
    "WISE_Matched_df['Group_e_VLSR'] = None\n",
    "WISE_Matched_df['Group_KDAR'] = None\n",
    "\n",
    "# Iterate over the rows of the DataFrame and store the group information\n",
    "print(\"Sources whose group name doesn't match:\")\n",
    "for i, row in WISE_Matched_df.iterrows():\n",
    "    group = row['Group']\n",
    "    if group == group:\n",
    "        matching_rows = WISE_Matched_df[WISE_Matched_df['Name'] == group]\n",
    "        \n",
    "        if not matching_rows.empty:\n",
    "            # Extract the matching row\n",
    "            match_row = matching_rows.iloc[0]\n",
    "\n",
    "            # Assign the values from the matching row\n",
    "            WISE_Matched_df.at[i, 'Group_VLSR'] = match_row['VLSR']\n",
    "            WISE_Matched_df.at[i, 'Group_e_VLSR'] = match_row['e_VLSR']\n",
    "            WISE_Matched_df.at[i, 'Group_KDAR'] = match_row['KDAR']\n",
    "        else:\n",
    "            print(i, group)\n",
    "\n",
    "####################### radio continuum detectuions ############################################################\n",
    "WISE_Matched_df.loc[(WISE_Matched_df['Catalog'] == 'no_radio') & (WISE_Matched_df['Magpis'] == 1), 'Catalog'] = 'observe'\n",
    "WISE_Matched_df.loc[(WISE_Matched_df['Catalog'] == 'no_radio') & (WISE_Matched_df['NVSS'] == 1), 'Catalog'] = 'observe'\n",
    "WISE_Matched_df.loc[(WISE_Matched_df['Catalog'] == 'no_radio') & (WISE_Matched_df['Cornish'] == 1), 'Catalog'] = 'observe'\n",
    "WISE_Matched_df.loc[(WISE_Matched_df['Catalog'] == 'no_radio') & (WISE_Matched_df['Yusef20cmGC'] == 1), 'Catalog'] = 'observe'\n",
    "WISE_Matched_df.loc[(WISE_Matched_df['Catalog'] == 'no_radio') & (WISE_Matched_df['LangRadio'] == 1), 'Catalog'] = 'observe'\n",
    "WISE_Matched_df.loc[(WISE_Matched_df['Catalog'] == 'no_radio') & (WISE_Matched_df['LangPaschen'] == 1), 'Catalog'] = 'observe'\n",
    "WISE_Matched_df.loc[(WISE_Matched_df['Catalog'] == 'no_radio') & (WISE_Matched_df['THOR'] == 1), 'Catalog'] = 'observe'\n",
    "WISE_Matched_df.loc[(WISE_Matched_df['Catalog'] == 'no_radio') & (WISE_Matched_df['SMGPS'] == 1), 'Catalog'] = 'observe'\n",
    "WISE_Matched_df.loc[(WISE_Matched_df['Catalog'] == 'no_radio') & (WISE_Matched_df['MeerKATGC'] == 1), 'Catalog'] = 'observe'\n",
    "\n",
    "############################### catalog ###################################################\n",
    "WISE_Matched_df.loc[WISE_Matched_df['VLSR']==WISE_Matched_df['VLSR'], 'Catalog'] = 'known'\n",
    "WISE_Matched_df.loc[WISE_Matched_df['Catalog']=='known', 'Catalog'] = 'K'\n",
    "WISE_Matched_df.loc[WISE_Matched_df['Catalog']=='group', 'Catalog'] = 'G'\n",
    "WISE_Matched_df.loc[WISE_Matched_df['Catalog']=='observe', 'Catalog'] = 'C'\n",
    "WISE_Matched_df.loc[WISE_Matched_df['Catalog']=='no_radio', 'Catalog'] = 'Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890ecd3f-0c6e-4b67-84aa-9eba3b44e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################verify that all sources are included################################3\n",
    "# prints regions in rrl_surveys that aren't used in matching\n",
    "df, unmatched = match_by_name(RRL_Surveys_df, WISE_df, extension = 'foo')\n",
    "unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4b866-0935-46be-8f6e-735857bff240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Te.  Since Wenger'19 uses the GName, have to finesse a little\n",
    "WISE_Matched_df['Name_hold'] = WISE_Matched_df['Name']\n",
    "WISE_Matched_df['Name'] = np.where(WISE_Matched_df['Name'].notnull(), WISE_Matched_df['Name'] + ';' + \n",
    "                                   WISE_Matched_df['GName'], WISE_Matched_df['GName'])\n",
    "df, unmatched = match_by_name(Te_df, WISE_Matched_df, extension = 'foo')\n",
    "WISE_Matched_df['Name'] = WISE_Matched_df['Name_hold']\n",
    "del WISE_Matched_df['Name_hold']\n",
    "unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3845bb-c73c-451e-9ced-f17329d0fa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints regions in Parallax_df that aren't used in matching\n",
    "df, unmatched = match_by_name(Parallax_df, WISE_df, name1 = 'GName', name2 = 'Parallax_Name', extension = 'foo')\n",
    "unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a1a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Compare old vs new ############################################3\n",
    "WISE_V23_df = pd.read_csv('/Users/loren/papers/wise/wise_hii_V2.3_hrds.csv')\n",
    "\n",
    "# Merging on different column names\n",
    "dfmerged = pd.merge(WISE_V23_df, WISE_Matched_df, on='GName', how='left')\n",
    "dfmerged['VLSR'] = pd.to_numeric(dfmerged['VLSR_x'], errors='coerce')\n",
    "\n",
    "# determine where new values are \"difference\" km/s from old ones, or there is now a value where there wasn't before, or vise-versa\n",
    "difference = 10\n",
    "for i in range(len(dfmerged)):\n",
    "    # Disregard multiple velocity sources and only include those with detections\n",
    "    if (dfmerged['VLSR_y'][i]>-230) & (dfmerged['VLSR2'][i]!=dfmerged['VLSR2'][i]) & (dfmerged['Year'][i]>=2021):\n",
    "        if (np.abs(dfmerged['VLSR'][i] - dfmerged['VLSR_y'][i]) > difference) or ((dfmerged['VLSR_x'][i]!=dfmerged['VLSR_x'][i]) or (dfmerged['VLSR_y'][i]!=dfmerged['VLSR_y'][i])):\n",
    "            print(dfmerged['Name_x'][i], dfmerged['VLSR_x'][i], dfmerged['Author_x'][i], \n",
    "                  dfmerged['Name_y'][i], dfmerged['VLSR_y'][i], dfmerged['Author_y'][i], i)\n",
    "            plt.scatter(dfmerged['GLong_x'][i], dfmerged['VLSR'][i] - dfmerged['VLSR_y'][i],s=5, c='r')\n",
    "        else:\n",
    "            plt.scatter(dfmerged['GLong_x'][i], dfmerged['VLSR'][i] - dfmerged['VLSR_y'][i],s=5, c='b')\n",
    "plt.plot([0, 360], [difference, difference])\n",
    "plt.plot([0, 360], [-difference, -difference])\n",
    "\n",
    "\n",
    "# check - removed erroneous old positions\n",
    "# TW044  8.5  Anderson et al. (2015b) G76.187+0.098;TW044 -4.81 Khan et al. (2024) 3613\n",
    "# ATCA802  -72.7  Anderson et al. (2015b) ATCA802;G340.247-00.373 -57.3 Wenger et al. (2021) 7203\n",
    "# ATCA810  -39.9  Anderson et al. (2015b) ATCA810;G342.438-00.061 -4.2 Wenger et al. (2021) 7324\n",
    "# Different sources - fixed\n",
    "#G302.504-00.749  31.0  Caswell & Haynes (1987) G302.481-00.035 -41.6 Wenger et al. (2021) 5510\n",
    "# original position in between two.  Khan position better.  Removed G028.295-00.377\n",
    "#G028.295-00.377  76.9  Anderson et al. (2015a) G28.287-0.364;G028.295-00.377 43.85 Khan et al. (2024) 1791\n",
    "# seems fine\n",
    "# G031.275+00.056  104.7  Lockman (1989) G31.279+0.063;G031.275+00.056 115.05 Khan et al. (2024) 2143\n",
    "# G033.418-00.004  76.5  Lockman (1989) G33.419-0.004;G033.418-00.004 63.49 Khan et al. (2024) 2275\n",
    "#FA099  11.1  Anderson et al. (2011) G7.177+0.088;FA099 0.49 Khan et al. (2024) 418\n",
    "# presumably better associations now\n",
    "#G338.450+00.061  -43.7000  Wenger et al. (2021) G338.450+00.061;G338.364-00.020;G338.430+00.048 -56.1 Wenger et al. (2021) 7072\n",
    "# GS003; SHRDS1241  -130.200  Wenger et al. (2021) GS003;SHRDS1241;G343.353-00.068;G343.352-00.081 -23.8 Wenger et al. (2021) 7361\n",
    "#   -24.5000  Wenger et al. (2021) G345.486+00.399;G345.338+01.442;G345.486+00.399 -8.3 Wenger et al. (2021) 7458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b5e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Determine if any associated sources are outside the region ##############################\n",
    "distance = ((WISE_Matched_df['GLong']-WISE_Matched_df['GLong_Observed'])**2 + (WISE_Matched_df['GLat']-WISE_Matched_df['GLat_Observed'])**2)**0.5\n",
    "distance[distance!=distance]=0\n",
    "wh = np.where(((distance*3600.) > ((WISE_Matched_df['Radius']**2)+(WISE_Matched_df['Resolution']*60.)**2)**0.5) & np.isfinite(WISE_Matched_df['GLong_Observed']))[0]\n",
    "print(WISE_Matched_df.loc[wh, ['GName', 'Name_Observed', 'GLong_Observed', 'GLat_Observed', 'Author']], distance[wh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39271b28",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WISE_Matched_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m################################## Numbers ##############################################\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(WISE_Matched_df)\n\u001b[1;32m      4\u001b[0m n_known \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(WISE_Matched_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatalog\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m n_group \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(WISE_Matched_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatalog\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WISE_Matched_df' is not defined"
     ]
    }
   ],
   "source": [
    "################################## Numbers ##############################################\n",
    "n = len(WISE_Matched_df)\n",
    "\n",
    "n_known = np.sum(WISE_Matched_df['Catalog'] == 'K')\n",
    "n_group = np.sum(WISE_Matched_df['Catalog'] == 'G')\n",
    "n_observe = np.sum(WISE_Matched_df['Catalog'] == 'C')\n",
    "n_no_radio = np.sum(WISE_Matched_df['Catalog'] == 'Q')\n",
    "print(n, n_known+n_group+n_observe+n_no_radio, n_known, n_group, n_observe, n_no_radio)\n",
    "\n",
    "n = len(WISE_V23_df)\n",
    "n_known = np.sum(WISE_V23_df['Catalog'] == ' K')\n",
    "n_group = np.sum(WISE_V23_df['Catalog'] == ' G')\n",
    "n_observe = np.sum(WISE_V23_df['Catalog'] == ' C')\n",
    "n_no_radio = np.sum(WISE_V23_df['Catalog'] == ' Q')\n",
    "print(n, n_known+n_group+n_observe+n_no_radio, n_known, n_group, n_observe, n_no_radio)\n",
    "\n",
    "merged_df = pd.merge(WISE_Matched_df, WISE_V23_df, on='GName', how='inner')\n",
    "#print(merged_df.head(10))\n",
    "\n",
    "diff_rows = merged_df.loc[(merged_df['Catalog_x'] != 'known') & (merged_df['Catalog_y']==' K')]\n",
    "cols = ['VLSR_x', 'Author_x', 'VLSR_y', 'Author_y']\n",
    "print(diff_rows[cols])\n",
    "\n",
    "diff_rows = merged_df.loc[(merged_df['Catalog_y'] != 'known') & (merged_df['Catalog_x']==' K')]\n",
    "cols = ['VLSR_x', 'Author_x', 'VLSR_y', 'Author_y']\n",
    "print(diff_rows[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c2455",
   "metadata": {},
   "source": [
    "# Now, let's create a SQL database from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2893e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WISE_Matched_df0 = WISE_Matched_df.copy()\n",
    "db_file = f'/Users/loren/papers/wise/python/WISE_HII_V{str_version}.db'\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "# Convert object columns to strings (this will handle most non-numeric data)\n",
    "for col in WISE_Matched_df0.select_dtypes(include='object').columns:\n",
    "    WISE_Matched_df0[col] = WISE_Matched_df0[col].astype(str)\n",
    "    \n",
    "# Write the DataFrame to the SQLite database as a new table\n",
    "table_name = f'WISE_HII_V{str_version}.db'\n",
    "WISE_Matched_df0.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "\n",
    "# Commit and close the connection to the database\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# Save the DataFrame to a pkl and csv file too\n",
    "WISE_Matched_df.to_pickle(f\"{dir_path}WISE_HII_V{str_version}.pkl\")\n",
    "WISE_Matched_df.to_csv(f\"{dir_path}WISE_HII_V{str_version}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9788bc97-4265-4b28-b98b-5037d1995150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog\n",
      "GLong\n",
      "GLat\n",
      "Radius\n",
      "Comments\n",
      "Group_Flag\n",
      "Group\n",
      "Name\n",
      "Alias\n",
      "Simbad_comments\n",
      "VGPSExclude\n",
      "MagpisExclude\n",
      "Parallax_Name\n",
      "Magpis\n",
      "NVSS\n",
      "Cornish\n",
      "Yusef20cmGC\n",
      "LangRadio\n",
      "LangPaschen\n",
      "THOR\n",
      "SMGPS\n",
      "MeerKATGC\n",
      "GName\n",
      "RA (J2000)\n",
      "Dec (J2000)\n",
      "Name_Split\n",
      "Name_Observed\n",
      "GName_Observed\n",
      "GLong_Observed\n",
      "GLat_Observed\n",
      "RA (J2000)_Observed\n",
      "Dec (J2000)_Observed\n",
      "TL\n",
      "e_TL\n",
      "FWHM\n",
      "e_FWHM\n",
      "VLSR\n",
      "e_VLSR\n",
      "VLSR_He\n",
      "e_VLSR_He\n",
      "FWHM_He\n",
      "e_FWHM_He\n",
      "TL_He\n",
      "e_TL_He\n",
      "VLSR_C\n",
      "e_VLSR_C\n",
      "FWHM_C\n",
      "e_FWHM_C\n",
      "TL_C\n",
      "e_TL_C\n",
      "Telescope\n",
      "Resolution\n",
      "Wavelength\n",
      "Frequency\n",
      "Author\n",
      "Year\n",
      "VLSR2\n",
      "e_VLSR2\n",
      "FWHM2\n",
      "e_FWHM2\n",
      "TL2\n",
      "e_TL2\n",
      "VLSR3\n",
      "e_VLSR3\n",
      "FWHM3\n",
      "e_FWHM3\n",
      "TL3\n",
      "e_TL3\n",
      "VLSR4\n",
      "e_VLSR4\n",
      "FWHM4\n",
      "e_FWHM4\n",
      "TL4\n",
      "e_TL4\n",
      "Name_Split_Observed\n",
      "Name_Te\n",
      "GName_Te\n",
      "GLong_Te\n",
      "GLat_Te\n",
      "RA (J2000)_Te\n",
      "Dec (J2000)_Te\n",
      "Author_Te\n",
      "Year_Te\n",
      "Te\n",
      "e_Te\n",
      "y\n",
      "e_y\n",
      "Name_Split_Te\n",
      "Name_Real_VLSR\n",
      "GName_Real_VLSR\n",
      "GLong_Real_VLSR\n",
      "GLat_Real_VLSR\n",
      "RA (J2000)_Real_VLSR\n",
      "Dec (J2000)_Real_VLSR\n",
      "Author_Real_VLSR\n",
      "Year_Real_VLSR\n",
      "Real_VLSR\n",
      "Name_Split_Real_VLSR\n",
      "Name_KDAR\n",
      "GName_KDAR\n",
      "GLong_KDAR\n",
      "GLat_KDAR\n",
      "RA (J2000)_KDAR\n",
      "Dec (J2000)_KDAR\n",
      "Author_KDAR\n",
      "Year_KDAR\n",
      "KDAR\n",
      "DMethod\n",
      "Name_Split_KDAR\n",
      "GLIMPSE_Flux\n",
      "GLIMPSE_e_Flux\n",
      "GLIMPSE_Area\n",
      "GLIMPSE_Saturation\n",
      "MIPSGal_Flux\n",
      "MIPSGal_e_Flux\n",
      "MIPSGal_Area\n",
      "MIPSGal_Saturation\n",
      "WISE3_Flux\n",
      "WISE3_e_Flux\n",
      "WISE3_Area\n",
      "WISE3_Saturation\n",
      "WISE4_Flux\n",
      "WISE4_e_Flux\n",
      "WISE4_Area\n",
      "WISE4_Saturation\n",
      "HiGal70_Flux\n",
      "HiGal70_e_Flux\n",
      "HiGal70_Area\n",
      "HiGal70_Saturation\n",
      "HiGal160_Flux\n",
      "HiGal160_e_Flux\n",
      "HiGal160_Area\n",
      "HiGal160_Saturation\n",
      "VGPS_Flux\n",
      "VGPS_e_Flux\n",
      "VGPS_Area\n",
      "VGPS_Saturation\n",
      "MAGPIS_Flux\n",
      "MAGPIS_e_Flux\n",
      "MAGPIS_Area\n",
      "GLong_Radio_Continuum\n",
      "GLat_Radio_Continuum\n",
      "RA (J2000)_Radio_Continuum\n",
      "Dec (J2000)_Radio_Continuum\n",
      "Author_Radio_Continuum\n",
      "Year_Radio_Continuum\n",
      "Match_Distance\n",
      "GLong_Molecular\n",
      "GLat_Molecular\n",
      "RA (J2000)_Molecular\n",
      "Dec (J2000)_Molecular\n",
      "VLSR_Molecular\n",
      "Molecule_Molecular\n",
      "Author_Molecular\n",
      "Year_Molecular\n",
      "GName_Parallax\n",
      "Alias_Parallax\n",
      "RA (J2000)_Parallax\n",
      "Dec (J2000)_Parallax\n",
      "Parallax\n",
      "e_Parallax\n",
      "mux\n",
      "e_mux\n",
      "muy\n",
      "e_muy\n",
      "VLSR_Parallax\n",
      "e_VLSR_Parallax\n",
      "Author_Parallax\n",
      "Name_Split_Parallax\n",
      "x\n",
      "Group_VLSR\n",
      "Group_e_VLSR\n",
      "Group_KDAR\n"
     ]
    }
   ],
   "source": [
    "cols = WISE_Matched_df.columns\n",
    "for col in cols:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665ca1e",
   "metadata": {},
   "source": [
    "# Unneeded (but useful) code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10664af",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################  Allows us to do name substitutions #################################    \n",
    "valid_names = ['G32.80+0.19',\n",
    "'G33.13-0.09',\n",
    "'G33.92+0.11',\n",
    "'G34.26+0.15',\n",
    "'G35.20-1.74',\n",
    "'G35.57-0.03',\n",
    "'G35.58+0.07',\n",
    "'G41.74+0.10',\n",
    "'G43.89-0.78',\n",
    "'G45.07+0.13',\n",
    "'G45.12+0.13',\n",
    "'G45.45+0.06',\n",
    "'G45.47+0.05',\n",
    "'G48.61+0.02',\n",
    "'G50.32+0.68',\n",
    "'G60.88-0.13',\n",
    "'G61.48+0.09',\n",
    "'G70.29+1.60',\n",
    "'G70.33+1.59',\n",
    "'G10.16-0.35',\n",
    "'G10.31-0.15',\n",
    "'G10.46+0.03',\n",
    "'G10.96+0.02',\n",
    "'G11.11-0.40',\n",
    "'G11.93-0.61',\n",
    "'G12.41+0.51',\n",
    "'G12.63-0.02',\n",
    "'G12.68+0.01',\n",
    "'G12.78+0.33',\n",
    "'G12.91-0.26',\n",
    "'G13.13-0.15',\n",
    "'G13.88+0.28',\n",
    "'G14.22-0.53',\n",
    "'G14.33-0.64',\n",
    "'G14.68-0.50',\n",
    "'G16.58-0.05',\n",
    "'G17.64+0.15',\n",
    "'G18.46-0.00',\n",
    "'G18.66-0.06',\n",
    "'G19.07-0.28',\n",
    "'G19.12-0.34',\n",
    "'G19.36-0.02',\n",
    "'G19.60-0.90',\n",
    "'G19.88-0.53',\n",
    "'G21.44-0.56',\n",
    "'G21.87+0.01',\n",
    "'G22.36+0.06',\n",
    "'G23.20-0.00',\n",
    "'G23.24-0.24',\n",
    "'G23.27+0.08',\n",
    "'G23.43-0.21',\n",
    "'G23.71-0.20',\n",
    "'G24.40-0.19',\n",
    "'G24.50-0.06',\n",
    "'G24.51-0.22',\n",
    "'G24.68-0.16',\n",
    "'G24.75-0.20',\n",
    "'G25.16+0.06',\n",
    "'G25.46-0.21',\n",
    "'G25.79-0.14',\n",
    "'G25.80+0.24',\n",
    "'G26.51+0.28',\n",
    "'G26.60-0.02',\n",
    "'G27.19-0.08',\n",
    "'G27.98+0.08',\n",
    "'G28.20-0.05',\n",
    "'G28.25+0.01',\n",
    "'G28.60-0.36',\n",
    "'G28.61+0.02',\n",
    "'G28.86+0.06',\n",
    "'G29.96-0.02',\n",
    "'G30.38-0.11',\n",
    "'G30.42+0.46',\n",
    "'G30.82+0.27',\n",
    "'G30.84-0.11',\n",
    "'G30.87+0.11',\n",
    "'G31.07+0.05',\n",
    "'G31.41+0.31',\n",
    "'G37.76-0.20',\n",
    "'G37.87-0.40',\n",
    "'G49.21-0.35',\n",
    "'G345.49+0.31',\n",
    "'G348.23-0.97',\n",
    "'G349.83-0.53',\n",
    "'G32.11+0.09',\n",
    "'G32.15+0.13',\n",
    "'G32.99+0.04',\n",
    "'G33.20-0.01',\n",
    "'G33.81-0.19',\n",
    "'G34.09+0.44',\n",
    "'G33.24+0.01',\n",
    "'G34.40+0.23',\n",
    "'G35.03+0.34',\n",
    "'G35.04-0.50',\n",
    "'G35.47+0.14',\n",
    "'G35.58-0.03',\n",
    "'G35.67-0.04',\n",
    "'G36.40+0.02',\n",
    "'G37.20-0.43',\n",
    "'G37.37-0.24',\n",
    "'G37.54-0.11',\n",
    "'G37.75-0.10',\n",
    "'G39.25-0.07',\n",
    "'G40.42+0.70',\n",
    "'G42.11-0.44',\n",
    "'G42.43-0.26',\n",
    "'G43.17+0.00',\n",
    "'G43.18-0.52',\n",
    "'G43.24-0.05',\n",
    "'G43.79-0.12',\n",
    "'G45.48+0.18',\n",
    "'G45.82-0.29',\n",
    "'G45.93-0.40',\n",
    "'G48.63+0.23',\n",
    "'G49.67-0.45',\n",
    "'G50.28-0.39',\n",
    "'G50.31+0.67',\n",
    "'G52.23+0.74',\n",
    "'G53.63+0.02',\n",
    "'G54.09-0.06',\n",
    "'G57.55-0.27',\n",
    "'G58.77+0.65',\n",
    "'G59.36-0.21',\n",
    "'G59.60+0.92',\n",
    "'G63.05-0.34',\n",
    "'G63.12+0.44']\n",
    "\n",
    "search_names = ['G032.800+00.190',\n",
    "'G033.130-00.090',\n",
    "'G033.920+00.110',\n",
    "'G034.260+00.150',\n",
    "'G035.200-01.740',\n",
    "'G035.570-00.030',\n",
    "'G035.580+00.070',\n",
    "'G041.740+00.100',\n",
    "'G043.890-00.780',\n",
    "'G045.070+00.130',\n",
    "'G045.120+00.130',\n",
    "'G045.450+00.060',\n",
    "'G045.470+00.050',\n",
    "'G048.610+00.020',\n",
    "'G050.320+00.680',\n",
    "'G060.880-00.130',\n",
    "'G061.480+00.090',\n",
    "'G070.290+01.600',\n",
    "'G070.330+01.590',\n",
    "'G010.160-00.350',\n",
    "'G010.310-00.150',\n",
    "'G010.460+00.030',\n",
    "'G010.960+00.020',\n",
    "'G011.110-00.400',\n",
    "'G011.930-00.610',\n",
    "'G012.410+00.510',\n",
    "'G012.630-00.020',\n",
    "'G012.680+00.010',\n",
    "'G012.780+00.330',\n",
    "'G012.910-00.260',\n",
    "'G013.130-00.150',\n",
    "'G013.880+00.280',\n",
    "'G014.220-00.530',\n",
    "'G014.330-00.640',\n",
    "'G014.680-00.500',\n",
    "'G016.580-00.050',\n",
    "'G017.640+00.150',\n",
    "'G018.460-00.000',\n",
    "'G018.660-00.060',\n",
    "'G019.070-00.280',\n",
    "'G019.120-00.340',\n",
    "'G019.360-00.020',\n",
    "'G019.600-00.900',\n",
    "'G019.880-00.530',\n",
    "'G021.440-00.560',\n",
    "'G021.870+00.010',\n",
    "'G022.360+00.060',\n",
    "'G023.200-00.000',\n",
    "'G023.240-00.240',\n",
    "'G023.270+00.080',\n",
    "'G023.430-00.210',\n",
    "'G023.710-00.200',\n",
    "'G024.400-00.190',\n",
    "'G024.500-00.060',\n",
    "'G024.510-00.220',\n",
    "'G024.680-00.160',\n",
    "'G024.750-00.200',\n",
    "'G025.160+00.060',\n",
    "'G025.460-00.210',\n",
    "'G025.790-00.140',\n",
    "'G025.800+00.240',\n",
    "'G026.510+00.280',\n",
    "'G026.600-00.020',\n",
    "'G027.190-00.080',\n",
    "'G027.980+00.080',\n",
    "'G028.200-00.050',\n",
    "'G028.250+00.010',\n",
    "'G028.600-00.360',\n",
    "'G028.610+00.020',\n",
    "'G028.860+00.060',\n",
    "'G029.960-00.020',\n",
    "'G030.380-00.110',\n",
    "'G030.420+00.460',\n",
    "'G030.820+00.270',\n",
    "'G030.840-00.110',\n",
    "'G030.870+00.110',\n",
    "'G031.070+00.050',\n",
    "'G031.410+00.310',\n",
    "'G037.760-00.200',\n",
    "'G037.870-00.400',\n",
    "'G049.210-00.350',\n",
    "'G345.490+00.310',\n",
    "'G348.230-00.970',\n",
    "'G349.830-00.530',\n",
    "'G032.110+00.090',\n",
    "'G032.150+00.130',\n",
    "'G032.990+00.040',\n",
    "'G033.200-00.010',\n",
    "'G033.810-00.190',\n",
    "'G034.090+00.440',\n",
    "'G033.240+00.010',\n",
    "'G034.400+00.230',\n",
    "'G035.030+00.340',\n",
    "'G035.040-00.500',\n",
    "'G035.470+00.140',\n",
    "'G035.580-00.030',\n",
    "'G035.670-00.040',\n",
    "'G036.400+00.020',\n",
    "'G037.200-00.430',\n",
    "'G037.370-00.240',\n",
    "'G037.540-00.110',\n",
    "'G037.750-00.100',\n",
    "'G039.250-00.070',\n",
    "'G040.420+00.700',\n",
    "'G042.110-00.440',\n",
    "'G042.430-00.260',\n",
    "'G043.170+00.000',\n",
    "'G043.180-00.520',\n",
    "'G043.240-00.050',\n",
    "'G043.790-00.120',\n",
    "'G045.480+00.180',\n",
    "'G045.820-00.290',\n",
    "'G045.930-00.400',\n",
    "'G048.630+00.230',\n",
    "'G049.670-00.450',\n",
    "'G050.280-00.390',\n",
    "'G050.310+00.670',\n",
    "'G052.230+00.740',\n",
    "'G053.630+00.020',\n",
    "'G054.090-00.060',\n",
    "'G057.550-00.270',\n",
    "'G058.770+00.650',\n",
    "'G059.360-00.210',\n",
    "'G059.600+00.920',\n",
    "'G063.050-00.340',\n",
    "'G063.120+00.440']\n",
    "\n",
    "# Create a dictionary that maps search names to valid names for substitution\n",
    "name_mapping = dict(zip(search_names, valid_names))\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv('/Users/loren/papers/wise/wise_hii_master_V3.0.csv')\n",
    "\n",
    "# Function to substitute names in the 'Name' column based on the mapping\n",
    "def substitute_names(name, name_mapping):\n",
    "    # Ensure that the name is a string (if it's not, return as is)\n",
    "    if not isinstance(name, str):\n",
    "        return name\n",
    "    \n",
    "    # For each name in the mapping, replace it in the string if found\n",
    "    for search_name, valid_name in name_mapping.items():\n",
    "        # If search_name exists in the current name, replace it with valid_name\n",
    "        if search_name in name:\n",
    "            name = name.replace(search_name, valid_name)\n",
    "    return name\n",
    "\n",
    "# Apply the substitution function to the 'Name' column\n",
    "df['Name'] = df['Name'].apply(substitute_names, name_mapping=name_mapping)\n",
    "df['Group'] = df['Group'].apply(substitute_names, name_mapping=name_mapping)\n",
    "\n",
    "df.to_csv('/Users/loren/papers/wise/modified_file.csv', index=False)\n",
    "\n",
    "print(\"File processed and saved as '/Users/loren/papers/wise/modified_file.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "953f5c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Catalog</th>\n",
       "      <th>GLong</th>\n",
       "      <th>GLat</th>\n",
       "      <th>Radius</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Group_Flag</th>\n",
       "      <th>Group</th>\n",
       "      <th>Name</th>\n",
       "      <th>Alias</th>\n",
       "      <th>Simbad_comments</th>\n",
       "      <th>...</th>\n",
       "      <th>NVSS</th>\n",
       "      <th>Cornish</th>\n",
       "      <th>Yusef20cmGC</th>\n",
       "      <th>LangRadio</th>\n",
       "      <th>LangPaschen</th>\n",
       "      <th>THOR</th>\n",
       "      <th>SMGPS</th>\n",
       "      <th>MeerKATGC</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sharpless</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.127687</td>\n",
       "      <td>206.696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S17;RCW138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>observe</td>\n",
       "      <td>0.008924</td>\n",
       "      <td>0.036389</td>\n",
       "      <td>24.9627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>known</td>\n",
       "      <td>0.020595</td>\n",
       "      <td>0.262560</td>\n",
       "      <td>444.387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>S17;RCW138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no_radio</td>\n",
       "      <td>0.025792</td>\n",
       "      <td>-0.231194</td>\n",
       "      <td>46.2579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no_radio</td>\n",
       "      <td>0.029622</td>\n",
       "      <td>-0.089272</td>\n",
       "      <td>31.5545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8584</th>\n",
       "      <td>observe</td>\n",
       "      <td>359.981000</td>\n",
       "      <td>-0.074288</td>\n",
       "      <td>17.7116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>no_radio</td>\n",
       "      <td>359.984000</td>\n",
       "      <td>-0.700351</td>\n",
       "      <td>72.9236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8586</th>\n",
       "      <td>observe</td>\n",
       "      <td>359.985000</td>\n",
       "      <td>0.027881</td>\n",
       "      <td>26.5407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G359.985+0.028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8587</th>\n",
       "      <td>observe</td>\n",
       "      <td>359.990000</td>\n",
       "      <td>0.012779</td>\n",
       "      <td>28.2032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8588</th>\n",
       "      <td>observe</td>\n",
       "      <td>359.997000</td>\n",
       "      <td>-0.175019</td>\n",
       "      <td>146.578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FA523</td>\n",
       "      <td>Peony Nebula</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8589 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Catalog       GLong      GLat   Radius Comments  Group_Flag  \\\n",
       "0     sharpless    0.003510  0.127687  206.696      NaN           0   \n",
       "1       observe    0.008924  0.036389  24.9627      NaN           0   \n",
       "2         known    0.020595  0.262560  444.387      NaN           0   \n",
       "3      no_radio    0.025792 -0.231194  46.2579      NaN           0   \n",
       "4      no_radio    0.029622 -0.089272  31.5545      NaN           0   \n",
       "...         ...         ...       ...      ...      ...         ...   \n",
       "8584    observe  359.981000 -0.074288  17.7116      NaN           0   \n",
       "8585   no_radio  359.984000 -0.700351  72.9236      NaN           0   \n",
       "8586    observe  359.985000  0.027881  26.5407      NaN           0   \n",
       "8587    observe  359.990000  0.012779  28.2032      NaN           0   \n",
       "8588    observe  359.997000 -0.175019  146.578      NaN           0   \n",
       "\n",
       "           Group            Name         Alias Simbad_comments  ...  NVSS  \\\n",
       "0            NaN      S17;RCW138           NaN             NaN  ...     0   \n",
       "1            NaN             NaN           NaN             NaN  ...     0   \n",
       "2     S17;RCW138             NaN           NaN             NaN  ...     0   \n",
       "3            NaN             NaN           NaN             NaN  ...     0   \n",
       "4            NaN             NaN           NaN             NaN  ...     0   \n",
       "...          ...             ...           ...             ...  ...   ...   \n",
       "8584         NaN             NaN           NaN             NaN  ...     0   \n",
       "8585         NaN             NaN           NaN             NaN  ...     0   \n",
       "8586         NaN  G359.985+0.028           NaN             NaN  ...     0   \n",
       "8587         NaN             NaN           NaN             NaN  ...     0   \n",
       "8588         NaN           FA523  Peony Nebula             NaN  ...     0   \n",
       "\n",
       "      Cornish Yusef20cmGC  LangRadio  LangPaschen  THOR  SMGPS  MeerKATGC  \\\n",
       "0           0           0          0            0     0      0          1   \n",
       "1           0           1          0            0     0      0          1   \n",
       "2           0           0          0            0     0      0          1   \n",
       "3           0           0          0            0     0      0          0   \n",
       "4           0           0          0            0     0      0          0   \n",
       "...       ...         ...        ...          ...   ...    ...        ...   \n",
       "8584        0           0          1            1     0      0          1   \n",
       "8585        0           0          0            0     0      0          1   \n",
       "8586        0           1          0            0     0      0          1   \n",
       "8587        0           0          0            1     0      0          1   \n",
       "8588        0           0          2            0     0      0          0   \n",
       "\n",
       "      Unnamed: 22  Unnamed: 23  \n",
       "0             NaN          NaN  \n",
       "1             NaN          NaN  \n",
       "2             NaN          NaN  \n",
       "3             NaN          NaN  \n",
       "4             NaN          NaN  \n",
       "...           ...          ...  \n",
       "8584          NaN          NaN  \n",
       "8585          NaN          NaN  \n",
       "8586          NaN          NaN  \n",
       "8587          NaN          NaN  \n",
       "8588          NaN          NaN  \n",
       "\n",
       "[8589 rows x 24 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deal with shrds\n",
    "shrds_df = pd.read_pickle('/Users/loren/catalogs/shrds/shrds.pkl')\n",
    "shrds_df = shrds_df.map(lambda x: x.value if isinstance(x, u.Quantity) else x)\n",
    "shrds_df\n",
    "df, unmatched = match_by_distance(WISE_df, shrds_df, size=WISE_df['Radius']/3600., extension='SHRDS', order_by='Year')\n",
    "df['GName_SHRDS'] = df['GName_SHRDS'].fillna('')\n",
    "\n",
    "non_unique_non_empty = df[df['GName_SHRDS'] != '']  # Filter out empty strings\n",
    "non_unique_non_empty = non_unique_non_empty[non_unique_non_empty['GName_SHRDS'].duplicated(keep=False)]\n",
    "\n",
    "for i, row in non_unique_non_empty.iterrows():\n",
    "    print(i+2, row['Catalog'], row['GLong'], row['Radius'], row['GName_SHRDS'], row['Match_Distance'], row['GLong_SHRDS'], row['GLat_SHRDS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd7166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the table if it exists to avoid the duplicate column issue\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"DROP TABLE IF EXISTS RRL_Surveys\")  # Drop the table to avoid conflict\n",
    "conn.commit()\n",
    "\n",
    "# Define the schema for the table based on the cleaned dataframe columns\n",
    "columns = RRL_Surveys_df.columns.tolist()\n",
    "column_definitions = ', '.join([f'\"{col}\" TEXT' if RRL_Surveys_df[col].dtype == 'object' else f'\"{col}\" REAL' for col in columns])\n",
    "\n",
    "# Create the table with the defined schema\n",
    "create_table_query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS RRL_Surveys (\n",
    "    {column_definitions}\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Insert the data from the cleaned DataFrame manually\n",
    "for row in RRL_Surveys_df.itertuples(index=False, name=None):\n",
    "    placeholders = ', '.join(['?'] * len(row))  # Generate correct number of placeholders\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO RRL_Surveys ({', '.join([f'\"{col}\"' for col in columns])})\n",
    "    VALUES ({placeholders});\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_query, row)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "print(\"Database populated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
